{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9524b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "import torch, numpy as np, os, clip, pickle as pkl, pandas as pd, numpy as np, urllib.request, ipywidgets as widgets\n",
    "from PIL import Image\n",
    "from IPython.display import display, Markdown, HTML, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bdf30f",
   "metadata": {},
   "source": [
    "### Load CLIP and Fashion Name Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d430c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "BASE_DIR = \"/home/ubuntu/AI_Search/fashion_data/\"\n",
    "\n",
    "\n",
    "with open(os.path.join(BASE_DIR, \"idx_to_pth_fashion_example_6_4_2023.pkl\"), \"rb\") as f:\n",
    "    idx_to_pth = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce8818",
   "metadata": {},
   "source": [
    "##### Create dictionaries for displaying fashion info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00fed4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(BASE_DIR, \"styles.csv\"), \"r\") as f:\n",
    "    csv_str = f.readlines()\n",
    "\n",
    "id_to_displayName = {}\n",
    "displayName_to_id = {}\n",
    "for csv_row in csv_str[1:]:\n",
    "    id_ = csv_row.split(\",\")[0]\n",
    "    display_name = csv_row.split(\",\")[-1].strip()\n",
    "    id_to_displayName[id_] = display_name\n",
    "    displayName_to_id[display_name] = id_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29db4905",
   "metadata": {},
   "source": [
    "## Launch Search Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3875862f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50845feff1bf4e14b1037d6b9f3c1a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', layout=Layout(width='400px'), placeholder='Natural Language Query'), Button(descâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a341c881b9224799a33ec1342a20c65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "No such metric",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 59\u001b[0m, in \u001b[0;36mon_button_clicked\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mExecutes image search when a button is clicked.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(query\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 59\u001b[0m     filepaths \u001b[38;5;241m=\u001b[39m \u001b[43mimage_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     output\u001b[38;5;241m.\u001b[39mclear_output()\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m output:\n",
      "Cell \u001b[0;32mIn [4], line 33\u001b[0m, in \u001b[0;36mimage_search\u001b[0;34m(query, n_results)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     31\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m768\u001b[39m\n\u001b[0;32m---> 33\u001b[0m     u \u001b[38;5;241m=\u001b[39m \u001b[43mAnnoyIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcosine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     u\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfashion_example_6_4_2023.ann\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \u001b[38;5;66;03m# super fast, will just map the file\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     text_features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_text(text)\n",
      "\u001b[0;31mValueError\u001b[0m: No such metric"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "clear_output()\n",
    "\n",
    "query = widgets.Text(placeholder=\"Natural Language Query\",layout=widgets.Layout(width='400px'))\n",
    "\n",
    "\n",
    "button_style = {'button_color': 'lightblue', 'font_weight': 'bold'}\n",
    "button = widgets.Button(description=\"Search\", style=button_style)\n",
    "output = widgets.Output()\n",
    "\n",
    "display(widgets.HBox([query, button],\n",
    "                     layout=widgets.Layout(justify_content='center')),\n",
    "        output)\n",
    "\n",
    "def image_search(query, n_results=6):\n",
    "    \"\"\"\n",
    "    Perform a natural language image search based on the given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string for the image search.\n",
    "        n_results (int, optional): The number of results to return. Defaults to 6.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of filepaths representing the top matching images.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    text = clip.tokenize(query).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        f = 768\n",
    "\n",
    "        u = AnnoyIndex(f, 'angular')\n",
    "        u.load(os.path.join(BASE_DIR, \"fashion_example_6_4_2023.ann\"))  # super fast, will just map the file\n",
    "\n",
    "        text_features = model.encode_text(text)\n",
    "\n",
    "        nns = u.get_nns_by_vector(text_features[0], n_results,search_k=-1, include_distances=False)\n",
    "        \n",
    "        filepaths = []\n",
    "        for i in range(0, len(nns)):\n",
    "            filepath = idx_to_pth[nns[i]].replace(\"/home/ubuntu/\", \"/home/ubuntu/AI_Search/\")\n",
    "            filepaths.append(filepath)   \n",
    "    \n",
    "    return filepaths\n",
    "\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    \"\"\"\n",
    "    Executes image search when a button is clicked.\n",
    "    \n",
    "    Parameters:\n",
    "        b (Button): The button that was clicked.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if len(query.value) > 0:\n",
    "        filepaths = image_search(query.value)\n",
    "        output.clear_output()\n",
    "        with output:\n",
    "            display_images_table(filepaths)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "def display_images_table(filepaths):\n",
    "    \"\"\"\n",
    "    Generates a table of images with their display names.\n",
    "\n",
    "    Parameters:\n",
    "        filepaths (List[str]): A list of filepaths of the images.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    table_rows = \"\"\n",
    "    num_columns = 3  # Number of columns in the table\n",
    "    num_images = len(filepaths)\n",
    "\n",
    "    # Calculate the number of rows based on the number of columns\n",
    "    num_rows = -(-num_images // num_columns)  # Equivalent to math.ceil(num_images / num_columns)\n",
    "\n",
    "    for i, filepath_og in enumerate(filepaths):\n",
    "        filepath = filepath_og.replace('/home/ubuntu/AI_Search/', '')\n",
    "        # print(\"filepath \", filepath)\n",
    "        displayName = id_to_displayName[filepath.split(\".\")[-2].split(\"/\")[-1]]\n",
    "\n",
    "        image_html = f'<img src=\"{filepath}\" style=\"max-width:200px; max-height:200px; display: block; margin: 0 auto; padding: 5%;\">'\n",
    "        table_rows += f'<td style=\"padding: 2%; text-align: center; cursor: pointer; background-color: lightblue;\" onmouseover=\"this.style.backgroundColor=\\'lightgrey\\';\" onmouseout=\"this.style.backgroundColor=\\'lightblue\\';\"><span style=\"font-weight: bold;\">{displayName}</span><br>{image_html}</td>'\n",
    "\n",
    "        # Add empty cells to make the table square for four images\n",
    "        if num_images == 4 and (i + 1) % num_columns == 0:\n",
    "            table_rows += '<td style=\"padding: 2%;\"></td>'\n",
    "\n",
    "        # Add a new row after each column\n",
    "        if (i + 1) % num_columns == 0:\n",
    "            table_rows = f'<tr>{table_rows}</tr>'\n",
    "\n",
    "    # Add an empty cell if the last row is incomplete\n",
    "    if num_images % num_columns != 0:\n",
    "        table_rows += '<td style=\"padding: 2%;\"></td>'\n",
    "\n",
    "    table_html = f'<table style=\"border-collapse: collapse; width: 100%;\">' \\\n",
    "                 f'<tr>{table_rows}</tr></table>'\n",
    "    display(HTML(table_html))\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "# dataset.observe(on_button_clicked, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c173c5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
